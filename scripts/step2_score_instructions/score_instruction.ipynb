{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"../../src\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.openai_lm import OpenAIModel\n",
    "from utils.parser import load_prompt, load_jsonl, save_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's a pleasure to meet you.\n",
      "\n",
      "What brings you here today?\n",
      "\n",
      "I'm happy to help you with any questions you have.\n",
      "\n",
      "Is there something specific you'd like to know or discuss?\n",
      "\n",
      "I'm all ears!\n",
      "\n",
      "Feel free to ask me anything.\n",
      "\n",
      "I'm here to assist you in any way I can.\n",
      "\n",
      "What's on your mind?\n",
      "\n",
      "I'm looking forward to our\n"
     ]
    }
   ],
   "source": [
    "from utils.openai_lm import OpenAIModel\n",
    "from utils.together_lm import TogetherModel\n",
    "\n",
    "# model_name = \"gpt-4-0314\"\n",
    "# model_name = \"gpt-35-turbo-0301\"\n",
    "# model_name = \"togethercomputer/llama-2-7b-chat\"\n",
    "model_name = \"togethercomputer/llama-2-70b-chat\"\n",
    "\n",
    "if \"gpt\" in model_name:\n",
    "    model = OpenAIModel(model_name=model_name, cache_file=None, model_mode=\"chat_gpt\", api_type=\"azure\")\n",
    "else:\n",
    "    model = TogetherModel(model_name=model_name, cache_file=None)\n",
    "\n",
    "# output, response = model.generate(\"Hello, how are you?\")\n",
    "output, response = model.generate([{\"role\": \"user\", \"content\": \"Hello, how are you?\"}])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instructions: 1000\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"prompt.txt\"\n",
    "# inst_path = f\"../../scripts/step1_generate_tasks/outputs/instructions.{model_name}.1000.txt\"\n",
    "inst_path = f\"../../scripts/step1_generate_instructions/outputs/instructions.gpt-4-0314.1000.txt\"\n",
    "\n",
    "prompt_template = load_prompt(prompt_path)\n",
    "inst_list = load_jsonl(inst_path)\n",
    "print(f\"Number of instructions: {len(inst_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instructions: 100\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_inst = len(inst_list)\n",
    "num_inst = 100\n",
    "random.seed(42)\n",
    "inst_list = random.sample(inst_list, num_inst)\n",
    "print(f\"Number of instructions: {len(inst_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for inst in inst_list:\n",
    "    # print(para)\n",
    "    instruction = inst[\"instruction\"]\n",
    "    prompt = prompt_template.substitute(instruction=instruction)\n",
    "    inst[\"input_messages\"] = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # input_messages_list.append([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # task_list.append(task)\n",
    "    # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:09<15:54,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [01:28<14:04,  9.60s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 37%|███▋      | 37/100 [06:17<10:19,  9.83s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 49%|████▉     | 49/100 [09:01<07:13,  8.49s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 58%|█████▊    | 58/100 [12:22<11:09, 15.94s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      "ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (1). Waiting 2 sec\n",
      " 65%|██████▌   | 65/100 [15:45<08:09, 14.00s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 66%|██████▌   | 66/100 [17:12<20:13, 35.68s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 67%|██████▋   | 67/100 [18:20<25:04, 45.58s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      "ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (1). Waiting 2 sec\n",
      " 70%|███████   | 70/100 [20:56<19:55, 39.85s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 72%|███████▏  | 72/100 [22:33<18:55, 40.54s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 73%|███████▎  | 73/100 [23:54<23:41, 52.64s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 86%|████████▌ | 86/100 [27:30<03:41, 15.81s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 88%|████████▊ | 88/100 [28:59<05:22, 26.85s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      "ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (1). Waiting 2 sec\n",
      " 91%|█████████ | 91/100 [31:54<05:48, 38.71s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 94%|█████████▍| 94/100 [33:21<02:47, 27.97s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      " 97%|█████████▋| 97/100 [35:01<01:16, 25.55s/it]ERROR:root:API error: 504 Server Error: Gateway Time-out for url: https://api.together.xyz/api/inference (0). Waiting 1 sec\n",
      "100%|██████████| 100/100 [36:33<00:00, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# output_path = f\"outputs/instructions_score.{model_name}.{num_inst}.txt\"\n",
    "output_path = f'outputs/instructions.gpt-4-0314.1000.score.{model_name.split(\"/\")[-1]}.txt'\n",
    "output_list = []\n",
    "for i, inst in enumerate(tqdm(inst_list)):\n",
    "    try: \n",
    "        input_messages = inst[\"input_messages\"]\n",
    "        output, response = model.generate(input_messages, max_tokens=2048, temperature=0.0)\n",
    "        # match Analysis: (xxx)\\nScore: (xxx)\n",
    "        match = re.search(r\"Analysis: (.*)\\nScore: ([0-9]+)\", output)\n",
    "        if not match:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = int(match.group(2))\n",
    "        output_list.append({\"score\": score, \"instruction\": inst[\"instruction\"], \"context\": inst[\"context\"]})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if i % 100 == 0:\n",
    "        save_jsonl(output_list, output_path)\n",
    "        print(f\"Number of outputs: {len(output_list)}\")\n",
    "save_jsonl(output_list, output_path)\n",
    "print(f\"Number of outputs: {len(output_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
